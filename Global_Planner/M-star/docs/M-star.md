# M-star 

论文阅读



## CBS 论文中对于 M* 的描述

M* 是一种与ID相关的算法。它是一种基于 A* 的算法，可根据冲突动态更改分支因子。通常，扩展节点仅生成一个子节点，其中每个机器人都朝着目标进行最佳的移动。这种情况一直持续到节点 n 处的 q ≥ 2 个机器人之间发生冲突。在这种情况下，需要在本地增加搜索维度。M* 从 n 向上追溯到 n 的所有祖先，直到根节点和所有这些节点都放回 OPEN 中。如果其中一个节点再次扩展，它将生成 bq 子节点，其中 q 冲突机器人进行所有可能的移动，而 k − q 非冲突的机器人进行最佳移动。



一种称为递归 M* （RM* ）  [51] 的增强版本将 q 冲突机器人划分为机器人子组，每个子组具有独立的冲突。然后，在每个组上递归调用 RM*。一种名为 ODRM* [17] 的变体将 Standley 的算子分解（参见第 3.3.7 节）结合在 RM* 之上。

## 摘要

多机器人路径规划很困难，因为 整个系统的状态空间 随着机器人数量的增加呈指数级增长。

所以 M* 会有两步操作：

1. 首先对每个机器人进行单独的规划
2. 然后对发生冲突的几组机器人进行路径重规划

从而最大限度地减少搜索空间的维度



提出了一种叫做 子维度扩展（subdimensional expansion）的通用策略：可以动态生成低维度的搜索空间嵌入到完整的状态空间中去。



然后还提出了一个机器人状态空间中的子维度扩展的算法实现，也就是 M*

可以把机器人运动的状态空间表示成一个 **图**







## 1 介绍



- 多机器人路径规划的根本问题是 **处理 高纬度的搜索空间**

- 多机器人路径规划算法可分为 **耦合** 和 **解耦** 两类
  - **耦合**（Coupled）表示算法考虑多个机器人之间的相互影响和依赖，也就是在完整的状态空间中寻找路径，优点是可以保证找到最佳的路径，但是对于多机器人系统来说计算量非常大，一般是不可行的
  - **解耦**（decoupled）表示算法将多个机器人的路径规划问题分解为单独的子问题，使得它们之间的影响降到最低。优点是可以快速地产生结果，但是不能保证找到最优解



- 针对上述问题，本文提出一种同时具有 耦合和解耦 方法的优点的算法，称之为子维度扩展（subdimensional expansion）。
- 在搜索空间中发现机器人之间的碰撞时，会在**局部增加空间的维度**，以构建一个最小的搜索空间，其中包含具有所需属性的路径。



**M* 算法** 是一种基于**A* 路径规划算法的子维度扩展方法**，适用于可以表示为图形的状态空间。作者证明了M* 算法是完备且最优的，然后通过模拟展示，相较于A* 算法，M*算法在寻找多机器人系统路径时需要的时间显著减少。





## 2 先前工作

先前存在许多算法，可以动态改变机器人的耦合方式，以达到规划目的。

- Krishna等人开发了一种用于机器人速度规划的分散式动态耦合方法。
  - 在他们的算法中，机器人首先尝试通过 ** 独立改变其速度 ** 来解决潜在的碰撞。如果这种方法不成功，那么参与碰撞的机器人会合作找到一个安全的速度计划。如果这也失败了，它们会招募未参与的机器人来改变它们的速度，以便找到一个解决方案。这种方法永远不会改变机器人所跟随的空间路径，因此既**不完备也不最优**。

- Clark等人引入了 ** 动态网络 ** ，明确地搜索不同大小的配置空间。对于能够相互通信的机器人组，计算联合计划。每当新的机器人加入组时，路径就会重新规划。这种方法 **会导致不必要的耦合**，因为并非所有能够通信的机器人都需要合作来找到安全路径，而且仅考虑局部交互。

- Van den Berg等人开发了一个 ** 规划时间算法 **，以找到一种耦合策略，该策略最小化了需要耦合的机器人集合的大小，以确保找到解决方案。这些机器人被限制为按顺序移动，这导致了哪些机器人必须在其他机器人之前或之后移动的约束。这些约束中的循环可以用来找到需要耦合规划的机器人集合。这种方法不太理想，因为它对机器人运动施加了限制，导致非最优路径，并且它执行的耦合具有全局性质。

- 机器学习领域也一直在努力确定** 何时需要耦合多个机器人 **。
  - Kok等人提出了一种方法，该方法对机器人**进行个体化的Q-learning**，但存储了探索的联合行动的奖励统计数据。如果这些统计数据表明在特定空间协调行动是有益的，那么算法就开始学习在该状态下进行协调行动。这种方法的好处是能够处理除了基本路径规划之外的任务，例如需要多个追捕者协调行动来捕捉目标。
  - Melo和Veloso开发了一种Q-learning算法，该算法将“协调”行动添加到每个机器人可用的行动集中，该行动利用最近邻机器人的状态来帮助选择要执行的行动。然后，只有当机器人学会执行协调行动时，机器人之间才发生协调。我们的工作**侧重于在搜索的上下文中进行动态耦合**，而不是强化学习。





## 3 子维度扩展

### A. 问题描述

任务目标是在一个共同的工作空间 $W$ 中为一组 $n$ 个机器人找到一个最优的、无碰撞的路径，从指定的初始配置到目标配置。我们用集合 $I = \{1, . . . , n\}$ 对机器人 $r_i$​ 进行索引。为了简洁起见，我们将用机器人的索引来代表机器人。我们使用 **上标** 来指示哪些机器人由一个集合、空间或元素描述，而 **下标** 则表示空间中的特定位置或集合中的特定元素。



每个机器人 $r_i$ 都有一个无障碍的 状态空间 $Q_i$ 。整个系统具有一个无障碍的 状态空间$Q = ∏_{i∈I} Q_i$，然而 $Q$ 中仍然存在导致不同机器人之间碰撞的状态。我们需要经常处理子空间，因此我们使用符号 $Q^Ω = ∏_{i∈Ω} Q_i $，来表示机器人子集 $Ω ⊂ I$ 的联合配置空间。这个符号也用于描述机器人子集的路径、机器人子集的位置等等。



由于每个点 $q_k ∈ Q$ 同时描述了每个机器人的位置，$Q$ 中的路径隐式地提供了时间协调。**如果成本函数取决于时间，时间可以被添加为机器人状态的一个元素。**这样做会导致最小的性能损失，因为所有机器人都具有相同的时间动态。因此，在搜索Q时，每个机器人的时间维度将折叠成一个单一的有效维度。



我们使用 $π(qk， ql)$ 来表示从 $q_k ∈ Q$  到 $q_l ∈ Q$  的路径上的点集，并使用 $π^∗(q_k， q_l)$ 来表示最小化指定成本函数的无冲突路径。我们的**目标是找到从初始状态 $q_I$ 到目标状态 $q_F$ 的最优无碰撞路径 $ π^∗(q_I ， q_F)$。**（**即从起始点到达目标点的最短无碰撞路径**）。



我们希望**最小化成本函数**  $f (π(q_k, q_l))$。我们假设完整状态空间中的路径成本等于**单个机器人的路径成本之和**：
$$
f(\pi(.))=\sum_{i\in I}f^i(\pi^i(.))\quad\pi^i(.)\subset Q^i
$$
如表达式（1）所示，保证了**成本函数可以分解为每个机器人的单独成本函数**。

对于在 $Q$ 中的路径 $π(q_I, q_F)$​，分别优化每个单独机器人路径的代价是最小的。虽然这样的路径几乎肯定会包含至少一个机器人之间的碰撞，但仍然有助于指导寻找一个无碰撞路径。



为了确保任何有限成本的路径都具有有限的长度（避免出现某条路径在终点附近徘徊），我们作如下要求：
$$
\exists\epsilon>0~s.t.~f^i(\pi^i(q_k^i,q_l^i))>\epsilon\quad q_k^i,q_l^i\in Q^i
$$


我们还定义了一个碰撞函数 $Ψ^{ij}$：
$$
\left.\Psi^{ij}(q^i,q^j)=\left\{\begin{array}{rl}\{i,j\},&A(q^i)\cap A(q^j)\neq\emptyset\\\emptyset,&\mathrm{otherwise}\end{array}\right.\right.
$$
简单来说，这个函数用于判断两个不同机器人 i 和 j 在给定的位置 $q_i$ 和 $q_j$ 时是否发生碰撞。如果$qi$和$qj$对应的空间有交集（即$A(q^i)$和$A(q^j)$不为空），则返回${i,j}$​，表示发生了碰撞；否则返回空集合表示没有发生碰撞。



然后我们还定义了一个全局的碰撞函数
$$
\Psi(q)=\bigcup_{i\neq j\in I}\Psi^{ij}(q^i,q^j)
$$


### B. 实现方法

子维度扩展利用满足(1)和(4)式的系统中机器人的自然解耦，来构造一个足够低维的搜索空间Q#，该空间嵌入在Q中。我们使用路径规划算法（称为规划器，也就是本文提出的 M*）来搜索 Q#。



在搜索Q#时，规划器会找到有关机器人之间碰撞的信息，然后利用这些信息来局部增加Q#的维度。通过这种方式，我们问题的结构定制搜索空间，允许我们可以搜索低维空间，同时保证所需的路径最终将存在于搜索空间中。



